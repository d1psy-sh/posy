https://www.lpalmieri.com/posts/2020-09-27-zero-to-production-4-are-we-observable-yet/#GDPR

bundle licenses:
 - cargo-about
 - cargo-bundle-licenses
 - cargo-lichking


git resolving:
keep git clones in cache
https://docs.rs/git2/latest/git2/struct.Repository.html#method.resolve_reference_from_short_name
https://docs.rs/git2/latest/git2/struct.Reference.html#method.peel_to_commit
https://docs.rs/git2/latest/git2/struct.Commit.html#method.id


quoting on posix: surround with ' and replace each internal ' with '"'"'


in .posy/ workspace, create .gitignore with '*' in it, so automatically gitignored


swap over to this for our newtypes?
https://docs.rs/aliri_braid/latest/aliri_braid/index.html
and also short strings


https://blog.yossarian.net/2022/05/09/A-most-vexing-parse-but-for-Python-packaging (and PEP 625 discussion)
https://discuss.python.org/t/pep-625-file-name-of-a-source-distribution/4686/27


switch to this for error reporting? https://docs.rs/miette/latest/miette/
can make pretty toml errors (https://docs.rs/toml_edit/latest/toml_edit/struct.TomlError.html)
but even better would be if can convince it to make pretty explanations of failed resolution! (well, might have to do this on our own)
maybe also look at https://docs.rs/ariadne/0.1.5/ariadne/

* storage
general Cache to KVDir or something?
transactional operations that take a closure? + a new version for putting stuff in a directory

fn get_or_set<K, F>(&self, key: &K, f: F) -> Result<impl Read + Seek>
  where K: CacheKey, F: FnOnce(impl Read + Seek) -> Result<()>

fn get_if_exists<K: CacheKey>(&self, key: &K) -> Option<impl Read + Seek>

// maybe this needs to return an object holding a lock?
fn read_through_dir<K: CacheKey, F>(&self, key: &K, f: F) -> Result<PathBuf>

...could also do it in a way where there's a separate handle for the lock, but idk if it's really useful

* artifact types

ArtifactRef -> (package, version) or (url)
  or... ReleaseRef?

Artifact -> Wheel/Pybi/Sdist, each a wrapper around a Read+Seek (might be file, might be lazy remote file...)
  methods to fetch metadtaa, unpack?


* PEP 643 (reliable metadata in sdists)
apparently this is a thing now! in an sdist, look in PKG-INFO in the root, and if Metadata-Version >= 2.2 and the fields we need are not listed in Dynamic: then we're good.

...and actually, I feel like a good resolution algorithm might be, trust PKG-INFO for all sdists, and then do the expensive prepare_metadata_for_build_wheel thing for all the unreliable sdists and replan if any of them turn out to have been wrong?

(the idea is that in most cases, the PKG-INFO will be reliable, so 99% of the time we can avoid building wheels for packages unless we're actually going to install them)

* check if we're using the same method of finding .dist-info as pip
https://github.com/pypa/pip/blob/bf91a079791f2daf4339115fb39ce7d7e33a9312/src/pip/_internal/utils/wheel.py#L84-L114

* better version ranges
I'm thinking: for each package, split available versions into three "tiers":
tier 1 (preferred): any "hinted" versions (like previously pinned version)
tier 2 (neutral): the regular non-yanked non-prerelease versions in order
tier 3 (dispreferred): all prereleases (some question about whether to consider yanked here too; or that could be tier 4)

within each tier, intern to get a vector of ints
version set is represented as 3 sets of ranges, one for each tier

when picking the next (package, version) to try, we always prefer candidates from a higher tier, so e.g. we never try any version from tier 3 until *all* packages have exhausted all their tier 1 and tier 2 options

...or, hmm. Does this actually work? when pubgrub asks us for the next (package, version) candidate assignment, then it restricts it to only a subset of packages. so we could be in a situation where the only valid candidates *from those packages* are pre-releases, b/c of the constraints set by the versions already chosen, even though there still exists some other resolution that doesn't use the pre-releases.

What this *might* do though is give the equivalent to "we only return pre-releases if explicitly requested"? ...ah, but no, if someone says `foo >= 10` and the only version >=10 is a pre-release, it could be selected. So this whole approach for pre-releases doesn't work.

* make resolution less wildly inefficient when choosing next candidate
[NOTE: there's a PR for this: https://github.com/pubgrub-rs/pubgrub/pull/104]

right now, every time pubgrub wants to consider a candidate, it gives us a set of ranges for all the packages under consideration, and then for every one, we do an O(n) loop through every package version, filtering out which ones fit into the range.
This is at least accidentally quadratic, quite possibly worse. There's gotta be a better data structure here.

One idea: with pubgrub custom Range trait support, have the range objects themselves aware of the complete version set and track which packages fit, propagating this incrementally through range operations?

(Or just storing the candidate versions sorted could also help quite a lot, b/c could make counting ~O(number of spans in range * log(n)) and "find max in range" in even ~O(log(n))

COMPLICATION: @ dependencies.
I think we ... actually cannot support these within pubgrub's model?
They make the intern-all-versions thing tricky of course, because they're new versions we can discover while we go
but even without that, they're... new versions we can discover as we go, which means that a set we previously told pubgrub was empty could suddenly become !empty, which could break the inferences it made from that, etc.
Fortunately, @ dependencies are supposed to be forbidden inside packages
so... say that @ dependencies are only supported at the top-level? must be specifically mentioned in pyproject.toml?
  prodigy-teams kind of case might want to also allow them in sibling projects within the same workspace
and then we can process them up-front, and simply tell pubgrub that these are the *only* versions available of those packages, the end.
(probably also want some kind of support for 'override' requirements there, which are regular dependencies that will usually be @ in practice, and that cause all other version constraints on that package to be *totally ignored*
